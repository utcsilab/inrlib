{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of INR with Complex-Valued Output\n",
    "\n",
    "Zach Stoebner  \n",
    "\n",
    "INRs typically have real-valued coordinates as inputs but potentially complex-valued outputs. What is the best way to handle complex-valued outputs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from typing import Optional, Tuple, Union, List\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "import inrlib\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 96\n",
    "train_dataset = inrlib.MRI3DDataset(RES=res,  # resolution of the image\n",
    "                            train=True, # use training set of coords\n",
    "                            shepp_or_atlas='shepp', # use shepp logan phantom\n",
    "                            complex=True, # use complex data\n",
    "                            )\n",
    "\n",
    "train_dataset.change_stage(train=True)\n",
    "val_dataset = deepcopy(train_dataset)\n",
    "val_dataset.change_stage(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((884736, 3),\n",
       " array([0.        , 0.01041667, 0.02083333, 0.03125   , 0.04166667,\n",
       "        0.05208333, 0.0625    , 0.07291667, 0.08333333, 0.09375   ,\n",
       "        0.10416667, 0.11458333, 0.125     , 0.13541667, 0.14583333,\n",
       "        0.15625   , 0.16666667, 0.17708333, 0.1875    , 0.19791667,\n",
       "        0.20833333, 0.21875   , 0.22916667, 0.23958333, 0.25      ,\n",
       "        0.26041667, 0.27083333, 0.28125   , 0.29166667, 0.30208333,\n",
       "        0.3125    , 0.32291667, 0.33333333, 0.34375   , 0.35416667,\n",
       "        0.36458333, 0.375     , 0.38541667, 0.39583333, 0.40625   ,\n",
       "        0.41666667, 0.42708333, 0.4375    , 0.44791667, 0.45833333,\n",
       "        0.46875   , 0.47916667, 0.48958333, 0.5       , 0.51041667,\n",
       "        0.52083333, 0.53125   , 0.54166667, 0.55208333, 0.5625    ,\n",
       "        0.57291667, 0.58333333, 0.59375   , 0.60416667, 0.61458333,\n",
       "        0.625     , 0.63541667, 0.64583333, 0.65625   , 0.66666667,\n",
       "        0.67708333, 0.6875    , 0.69791667, 0.70833333, 0.71875   ,\n",
       "        0.72916667, 0.73958333, 0.75      , 0.76041667, 0.77083333,\n",
       "        0.78125   , 0.79166667, 0.80208333, 0.8125    , 0.82291667,\n",
       "        0.83333333, 0.84375   , 0.85416667, 0.86458333, 0.875     ,\n",
       "        0.88541667, 0.89583333, 0.90625   , 0.91666667, 0.92708333,\n",
       "        0.9375    , 0.94791667, 0.95833333, 0.96875   , 0.97916667,\n",
       "        0.98958333]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flattened coordinate array and unique x coorindates\n",
    "\n",
    "coords = train_dataset.x_data\n",
    "coords.shape, np.unique(coords[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> complex128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 95.5, 95.5, -0.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = train_dataset.image\n",
    "print(type(image), image.dtype)\n",
    "\n",
    "plt.imshow(np.abs(image[..., res//2]), cmap='gray')\n",
    "plt.title(f'{image.shape}')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = inrlib.GenericDataModule(batch_size=None,\n",
    "                                   num_workers=8, \n",
    "                                   use_worker_init_fn=True,\n",
    "                                   train=train_dataset, \n",
    "                                   val=val_dataset, # not necessary\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss subclass\n",
    "\n",
    "from typing import Mapping\n",
    "\n",
    "\n",
    "class ComplexINRLoss(inrlib.ABCLoss):\n",
    "    def set_params(self, *args, **kwargs):\n",
    "        '''\n",
    "        Set additional optimization parameters for loss, i.e., clean reconstruction\n",
    "        '''\n",
    "        return\n",
    "    \n",
    "    def reconstruct_params(self, image_shape, outputs, **other) -> Mapping[str, np.ndarray]:\n",
    "        '''\n",
    "        If added additional parameters, reconstruct them here \n",
    "        '''\n",
    "        return {**other}\n",
    "    \n",
    "    def prepare_input(self,  x: torch.Tensor, y: torch.Tensor, **other) -> Mapping[str, torch.Tensor]:\n",
    "        xi = x.clone()\n",
    "        yi = inrlib.make_complex(y)\n",
    "        yi = inrlib.make_real(yi)\n",
    "        return {'x': xi, 'y': yi, **other}\n",
    "    \n",
    "    def prepare_output(self,  y_hat: torch.Tensor, y: torch.Tensor, **other) -> Mapping[str, torch.Tensor]:\n",
    "        return {'pred': torch.squeeze(y_hat), \n",
    "                'target': torch.squeeze(y), \n",
    "                **other}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "constr = inrlib.ComplexImaginaryConstraint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = inrlib.L1Regularizer(dim=None, \n",
    "                        weight=1e-4, \n",
    "                        constraints=[constr]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose loss function\n",
    "\n",
    "loss_fn = ComplexINRLoss('complex_mse', # loss type can be anything\n",
    "                    regularizers=[reg], \n",
    "                    fncs=[inrlib.MSELoss()],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posenc = inrlib.GaussianPosEnc(d_input=3, # dimensionality of input ie (x,y,z)\n",
    "                            embed_sz=256, # number of features to lift to\n",
    "                            scale=2. # scaling factor of the gaussian\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = inrlib.NeuralImplicitMLP(\n",
    "    lr=1e-3, # learning rate\n",
    "    posenc=posenc, # positional encoding\n",
    "    n_features=256, # input dimension\n",
    "    n_layers=16, # network depth\n",
    "    n_output=2,  # complex output\n",
    "    \n",
    "    ###\n",
    "    loss_fn=loss_fn, # loss function\n",
    "    act_fn=nn.LeakyReLU(), # activation function\n",
    "    norm_fn=nn.Identity(), # normalization function\n",
    "    output_fn=nn.Identity(), # no output activation -> unbounded output\n",
    "    ###\n",
    "    \n",
    "    optimizer=torch.optim.Adam,\n",
    "    metrics=[inrlib.NRMSELoss()] # metrics to track besides loss, min, max\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_transf = inrlib.PhaseTransform(cmap='gray', vmin=-3.14, vmax=3.14)\n",
    "mag_transf = inrlib.MagnitudeTransform(cmap='gray')\n",
    "view_transfs = [phase_transf, mag_transf]\n",
    "\n",
    "img_logger = inrlib.NeuralImplicitImageLogger(view_transforms=view_transfs, # callables to transform output for viewing, i.e., phase and magnitude for complex-valued images\n",
    "                                        save_freq=10, # how often to save images\n",
    "                                        best_only=True, # only save best images\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = CSVLogger(save_dir='logs/shepp', \n",
    "                   name='complex_shepp_demo', \n",
    "                   version=1) # not needed, but useful for logging\n",
    "bar = TQDMProgressBar(refresh_rate=1000)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, \n",
    "                     log_every_n_steps=1, \n",
    "                     benchmark=True,\n",
    "                     accumulate_grad_batches=2,\n",
    "                     logger=logger,\n",
    "                     callbacks=[img_logger, bar], \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zstoebs/research/inrlib/inrlib/utils/visualization.py:28: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  img = (img * 255).astype(np.uint8)\n",
      "/home/zstoebs/research/inrlib/inrlib/utils/visualization.py:28: RuntimeWarning: invalid value encountered in cast\n",
      "  img = (img * 255).astype(np.uint8)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | posenc     | GaussianPosEnc | 1.0 K \n",
      "1 | loss_fn    | ComplexINRLoss | 0     \n",
      "2 | act_fn     | LeakyReLU      | 0     \n",
      "3 | norm_fn    | Identity       | 0     \n",
      "4 | output_fn  | Identity       | 0     \n",
      "5 | base_model | Sequential     | 1.1 M \n",
      "----------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "1.0 K     Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.216     Total estimated model params size (MB)\n",
      "/home/zstoebs/anaconda3/envs/nerf/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory logs/shepp/complex_shepp_demo/version_1 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898a898f931e4bf39ada9947bed402ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cfebc47d3244ca84a8dd938c022411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cc87b4c881453185d6bcc5c2f4520d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zstoebs/anaconda3/envs/nerf/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=MLP, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run complex floats are not implemented by PyTorch on GPU... :( So we have to keep complex outputs as 2-channel real until after backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
