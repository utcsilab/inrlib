{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo usage of INR framework\n",
    "\n",
    "Zach Stoebner\n",
    "\n",
    "This noteboook runs through a 3D example on CT Shepp-Logan phantom. The framework provides abstract base classes for the components for training an INR. These are: \n",
    "- `ABCDataset`\n",
    "- `ABCPosEnc`\n",
    "- `ABCModel`\n",
    "- `ABCLoss`\n",
    "- `ABCRegularizer`\n",
    "- `ABCTransform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from typing import Optional, Tuple, Union, List\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "import inrlib\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "With Lightning, this framework expects 2 classes: the Dataset and the DataModule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For training, the coordinates are the dataset and are mapped to the range [0,1]. For testing, we can use  coordinates in between the training samples since these functions are intended to predict continuity from discretization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 96\n",
    "train_dataset = inrlib.MRI3DDataset(RES=res,  # resolution of the image\n",
    "                            train=True, # use training set of coords\n",
    "                            shepp_or_atlas='shepp' # use shepp logan phantom\n",
    "                            )\n",
    "\n",
    "train_dataset.change_stage(train=True)\n",
    "val_dataset = deepcopy(train_dataset)\n",
    "val_dataset.change_stage(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((884736, 3),\n",
       " array([0.        , 0.01041667, 0.02083333, 0.03125   , 0.04166667,\n",
       "        0.05208333, 0.0625    , 0.07291667, 0.08333333, 0.09375   ,\n",
       "        0.10416667, 0.11458333, 0.125     , 0.13541667, 0.14583333,\n",
       "        0.15625   , 0.16666667, 0.17708333, 0.1875    , 0.19791667,\n",
       "        0.20833333, 0.21875   , 0.22916667, 0.23958333, 0.25      ,\n",
       "        0.26041667, 0.27083333, 0.28125   , 0.29166667, 0.30208333,\n",
       "        0.3125    , 0.32291667, 0.33333333, 0.34375   , 0.35416667,\n",
       "        0.36458333, 0.375     , 0.38541667, 0.39583333, 0.40625   ,\n",
       "        0.41666667, 0.42708333, 0.4375    , 0.44791667, 0.45833333,\n",
       "        0.46875   , 0.47916667, 0.48958333, 0.5       , 0.51041667,\n",
       "        0.52083333, 0.53125   , 0.54166667, 0.55208333, 0.5625    ,\n",
       "        0.57291667, 0.58333333, 0.59375   , 0.60416667, 0.61458333,\n",
       "        0.625     , 0.63541667, 0.64583333, 0.65625   , 0.66666667,\n",
       "        0.67708333, 0.6875    , 0.69791667, 0.70833333, 0.71875   ,\n",
       "        0.72916667, 0.73958333, 0.75      , 0.76041667, 0.77083333,\n",
       "        0.78125   , 0.79166667, 0.80208333, 0.8125    , 0.82291667,\n",
       "        0.83333333, 0.84375   , 0.85416667, 0.86458333, 0.875     ,\n",
       "        0.88541667, 0.89583333, 0.90625   , 0.91666667, 0.92708333,\n",
       "        0.9375    , 0.94791667, 0.95833333, 0.96875   , 0.97916667,\n",
       "        0.98958333]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flattened coordinate array and unique x coorindates\n",
    "\n",
    "coords = train_dataset.x_data\n",
    "coords.shape, np.unique(coords[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 95.5, 95.5, -0.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# middle slice along z\n",
    "\n",
    "image = train_dataset.image\n",
    "print(type(image), image.dtype)\n",
    "\n",
    "plt.imshow(image[..., res//2], cmap='gray')\n",
    "plt.title(f'{image.shape}')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule\n",
    "\n",
    "The provided datamodule only needs a training image to train and validate. If no special validation dataset is provided, the training dataset will be copied and changed to *non-training*, expecting the datasets to be derived from `ABCDataset`. \n",
    "\n",
    "*Non-training implies that the input coordinates will be shifted to the midpoint of the training coordinates.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = inrlib.GenericDataModule(batch_size=None,\n",
    "                                   num_workers=8, \n",
    "                                   use_worker_init_fn=True,\n",
    "                                   train=train_dataset, \n",
    "                                   val=val_dataset, # not necessary\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "There are 2 key parts to an implicit neural representation: \n",
    "1. A lifting operation that encodes data at each position in a higher-dimensional space\n",
    "2. A multilayer perceptron (MLP) that maps the positional encoding to the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "[Tancik et al. 2020](https://arxiv.org/abs/2006.10739) found that a Gaussian encoder works well generally and demoed this in the complex 3D case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posenc = inrlib.GaussianPosEnc(d_input=3, # dimensionality of input ie (x,y,z)\n",
    "                            embed_sz=256, # number of features to lift to\n",
    "                            scale=2. # scaling factor of the gaussian\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "A system is provided for custom loss functions for constraint via regularization using the following base class: \n",
    "\n",
    "```python\n",
    "class ABCLoss(ABC, nn.Module):\n",
    "    def __init__(self, loss_type, d_input: int, fncs: List[nn.Module] = [nn.MSELoss()], regularizers: List[nn.Module]=[], **kwargs):\n",
    "        super().__init__()\n",
    "        assert len(fncs) > 0, 'Specify at least one loss function'\n",
    "        self.fncs = fncs\n",
    "        self.loss_type = loss_type  # type of loss\n",
    "        self.d_input = d_input  # dimension of input (x) to model\n",
    "        self.sample_inds = []  # indexing values if data contains multiple samples per image\n",
    "        self.regularizers = regularizers\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        loss = 0\n",
    "        for fnc in self.fncs:\n",
    "            loss += fnc(pred,target)\n",
    "        \n",
    "        reg_val = 0\n",
    "        for reg in self.regularizers:\n",
    "            reg_val += reg(pred)\n",
    "\n",
    "        return loss + reg_val\n",
    "    \n",
    "    @abstractmethod\n",
    "    def set_params(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reconstruct_params(self) -> Mapping[str, np.ndarray]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def prepare_input(self) -> Mapping[str, torch.Tensor]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def prepare_output(self) -> Mapping[str, torch.Tensor]:\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss subclass\n",
    "\n",
    "from typing import Mapping\n",
    "\n",
    "\n",
    "class myINRLoss(inrlib.ABCLoss):\n",
    "    def set_params(self, *args, **kwargs):\n",
    "        '''\n",
    "        Set additional optimization parameters for loss, i.e., clean reconstruction\n",
    "        '''\n",
    "        return\n",
    "    \n",
    "    def reconstruct_params(self, image_shape, outputs, **other) -> Mapping[str, np.ndarray]:\n",
    "        '''\n",
    "        If added additional parameters, reconstruct them here \n",
    "        '''\n",
    "        return {**other}\n",
    "    \n",
    "    def prepare_input(self,  x: torch.Tensor, y: torch.Tensor, **other) -> Mapping[str, torch.Tensor]:\n",
    "        xi = x.clone()\n",
    "        yi = inrlib.make_complex(y.clone())\n",
    "        yi = torch.view_as_real(yi) if torch.is_complex(yi) else yi\n",
    "        return {'x': xi, 'y': yi, **other}\n",
    "    \n",
    "    def prepare_output(self,  y_hat: torch.Tensor, y: torch.Tensor, **other) -> Mapping[str, torch.Tensor]:\n",
    "        return {'pred': y_hat.reshape(*y.shape), 'target': y, **other}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints\n",
    "\n",
    "These callables enforce properties over the model output that are fed as input to the regularizer. \n",
    "\n",
    "For example, a constraint on the magnitude of a complex-valued output to be less than or equal to 1 would look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "constr = inrlib.ComplexImaginaryConstraint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "These callables enforce regularization over the model's output. \n",
    "\n",
    "For example, an $L_2$ regularization over the constraint defined above would look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = inrlib.L2Regularizer(dim=None, \n",
    "                        weight=1e-4, \n",
    "                        constraints=[constr]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: using this regularizer is for demo purposes only and will regularize away from the optimum, i.e., the magnitude of the complex-valued image should not be less than or equal to 1.*\n",
    "\n",
    "Now, compose the custom loss function... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose loss function\n",
    "\n",
    "loss_fn = myINRLoss('mse', # loss type can be anything\n",
    "                    regularizers=[reg], \n",
    "                    fncs=[inrlib.MSELoss()],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "Models can inherit from the NeuralImplicit base class: \n",
    "\n",
    "```python\n",
    "class NeuralImplicit(ABC, pl.LightningModule):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.val_outputs = []\n",
    "\t\tself.scores = None\n",
    "  \n",
    "\tdef on_validation_start(self):\n",
    "\t\tself.val_outputs = []\n",
    "\t\tself.scores = None\n",
    "\t\t\n",
    "\t@abstractmethod\n",
    "\tdef reconstruct(self):\n",
    "\t\tpass\n",
    "\t\n",
    "\t@abstractmethod\n",
    "\tdef compute_metrics(self):\n",
    "\t\tpass\n",
    "```\n",
    "\n",
    "Derived classes should implement reconstruction (called externally by logger to reshape back to image dimension) and metric computation (called every iteration to compute loss and save state). \n",
    "\n",
    "The provided general neural implicit MLP implementation performs basic reconstruction given the corresponding indices from the dataset and the target and prediction values, reshaping the arrays back to the image shape from dataset. The loss, min, max, and stddev of the target and prediction are automatically computed for comparison and additional callables to compute metrics of the form `metric(pred, target)` can be added in a list. \n",
    "\n",
    "*Computation*: Since the dimensionality of the domain is $96^3 * 3 = 884,736 * 3$, we ideally need just as many parameters. For 256-width layers, this would mean 15,552 layers needed. However, the image has underlying structure and positional encoding is intended to highlight periodic structure to ease computation so we can get away with a much smaller network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = inrlib.NeuralImplicitMLP(\n",
    "    lr=1e-3, # learning rate\n",
    "    posenc=posenc, # positional encoding\n",
    "    n_features=256, # input dimension\n",
    "    n_layers=16, # network depth\n",
    "    n_output=2,  # complex 2-channel\n",
    "    \n",
    "    ###\n",
    "    loss_fn=loss_fn, # loss function\n",
    "    act_fn=nn.LeakyReLU(), # activation function\n",
    "    norm_fn=nn.Identity(), # normalization function\n",
    "    output_fn=nn.Identity(), # no output activation -> unbounded output\n",
    "    ###\n",
    "    \n",
    "    optimizer=torch.optim.Adam,\n",
    "    metrics=[inrlib.NRMSELoss()] # metrics to track besides loss, min, max\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging\n",
    "\n",
    "With Lightning, we can define an image logging callback to log images, CSVs, etc. during training and validation. \n",
    "\n",
    "The provided image logging implementation saves images with metrics at the end of validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_transf = inrlib.PhaseTransform(cmap='gray', vmin=-3.14, vmax=3.14)\n",
    "mag_transf = inrlib.MagnitudeTransform(cmap='gray')\n",
    "view_transfs = [phase_transf, mag_transf]\n",
    "\n",
    "img_logger = inrlib.NeuralImplicitImageLogger(view_transforms=view_transfs, # callables to transform output for viewing, i.e., phase and magnitude for complex-valued images\n",
    "                                        save_freq=10, # how often to save images\n",
    "                                        best_only=True, # only save best images\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = CSVLogger(save_dir='logs/shepp', \n",
    "                   name='shepp_demo', \n",
    "                   version=1)\n",
    "bar = TQDMProgressBar(refresh_rate=1000)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, \n",
    "                     log_every_n_steps=1, \n",
    "                     benchmark=True,\n",
    "                     accumulate_grad_batches=2,\n",
    "                     logger=logger,\n",
    "                     callbacks=[img_logger, bar], \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zstoebs/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:198: Experiment logs directory logs/shepp/shepp_demo/version_1 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/zstoebs/research/inrlib/inrlib/utils/visualization.py:28: RuntimeWarning: invalid value encountered in cast\n",
      "  img = (img * 255).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | posenc     | GaussianPosEnc | 1.0 K \n",
      "1 | loss_fn    | myINRLoss      | 0     \n",
      "2 | act_fn     | LeakyReLU      | 0     \n",
      "3 | norm_fn    | Identity       | 0     \n",
      "4 | output_fn  | Identity       | 0     \n",
      "5 | base_model | Sequential     | 1.1 M \n",
      "----------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "1.0 K     Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.216     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]SHAPE torch.Size([884736, 2]) torch.Size([884736, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zstoebs/anaconda3/envs/inr/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([884736, 1, 2])) that is different to the input size (torch.Size([884736, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5832.00 GiB. GPU 0 has a total capacity of 44.35 GiB of which 38.02 GiB is free. Including non-PyTorch memory, this process has 448.00 MiB memory in use. Process 1205149 has 5.88 GiB memory in use. Of the allocated memory 99.90 MiB is allocated by PyTorch, and 34.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMLP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1062\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/inrlib/inrlib/models/MLP.py:145\u001b[0m, in \u001b[0;36mNeuralImplicitMLP.validation_step\u001b[0;34m(self, batch, batch_idx, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    143\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m: y_hat, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch}\n\u001b[0;32m--> 145\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dict(scores, sync_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [{key: val\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems()}]\n",
      "File \u001b[0;32m~/research/inrlib/inrlib/models/MLP.py:96\u001b[0m, in \u001b[0;36mNeuralImplicitMLP.compute_metrics\u001b[0;34m(self, pred, target, stage, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     91\u001b[0m                     pred: torch\u001b[38;5;241m.\u001b[39mTensor, \n\u001b[1;32m     92\u001b[0m                     target: torch\u001b[38;5;241m.\u001b[39mTensor, \n\u001b[1;32m     93\u001b[0m                     stage: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m     94\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 96\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pred_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mmean(pred),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_target_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mmax(target),\n\u001b[1;32m    108\u001b[0m     }\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/research/inrlib/inrlib/losses/__init__.py:25\u001b[0m, in \u001b[0;36mABCLoss.forward\u001b[0;34m(self, pred, target, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fnc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfncs:\n\u001b[0;32m---> 25\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mfnc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m reg_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregularizers:\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/inr/lib/python3.10/site-packages/torch/nn/functional.py:3339\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3336\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3338\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m-> 3339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5832.00 GiB. GPU 0 has a total capacity of 44.35 GiB of which 38.02 GiB is free. Including non-PyTorch memory, this process has 448.00 MiB memory in use. Process 1205149 has 5.88 GiB memory in use. Of the allocated memory 99.90 MiB is allocated by PyTorch, and 34.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=MLP, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results below show that the NRMSE decreased after only 10 epochs and some structure in the prediction is starting to appear. However, as noted above, the loss regularizes away from the optimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='assets/shepp/fit_MagnitudeTransform_e=0_v=0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='assets/shepp/fit_PhaseTransform_e=0_v=0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='assets/shepp/fit_MagnitudeTransform_e=9_v=10.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='assets/shepp/fit_PhaseTransform_e=9_v=10.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
